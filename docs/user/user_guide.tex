\documentclass[letterpaper,12pt]{article} %amsart
\usepackage[raggedright]{titlesec}
\usepackage[margin=1in]{geometry}
%\renewcommand{\thesection}{\Large{\textbf{section}}}
%\renewcommand{\thesubsection}{\Alph{subsection}}
\usepackage{indentfirst}
\usepackage{secdot}
\usepackage{titlesec}
\usepackage{xcolor, soul}
\usepackage{mathabx}
\usepackage{enumitem}
\usepackage{times}
\usepackage{float}
\usepackage{parcolumns}


\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newfloat{Code}{H}{myc}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=2pt,
    firstnumber=1,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    xleftmargin=.8cm,
    xrightmargin=.8cm
}
\lstset{style=mystyle}
\usepackage{graphicx}
\graphicspath{ {./img/} }
\sethlcolor{yellow}
\sectiondot{subsection}
\setlength{\parindent}{1em}
\raggedbottom %*fix for extra gap between paragraphs in ML section

\titleformat{\section}{\centering\uppercase}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\itshape}{\thesubsection.}{0.5em}{}

\renewenvironment{abstract}{\textbf{\textit{Abstract---}}}{}

\begin{document}
\title{\Large{\textbf{OpenMP for Python User Guide}}}
\author{Caleb Huck}
\date{June 3rd, 2021 ************ change}
\maketitle
%\clearpage

\section{Installation}
\hl{pip instal (go through process on vm)l}
We have tried to make the install process of our software as easy and straight-forward as possible. The software can be installed in any directory. The only requirement for it to work is a single environment variable and a directory that must be included in the PATH environment variable. Below is the complete list of steps needed to get up and running on Windows, Mac, or Linux:
\begin{enumerate}
\item Make sure that Java 8 or later is installed on the system. This can be checked by running the command \texttt{java -version} in the terminal
\item Open a terminal in the directory where you want to download the software (this can be any directory) and run the command: \\ \texttt{git clone https://github.com/calebhuck/OpenMPy.git .}
\item Create an environment variable called JYTHON\_HOME and set to value to be the top level directory of the file structure you just downloaded (the directory where jython-dev.jar resides)
\item Add the \$JYTHON\_HOME/bin/ folder to your current PATH environment variable (this is where the launcher script for Jython resides)
\item Open a new terminal and run the command \texttt{jython --version}. If everything was done correctly, the output should be \texttt{Jython 2.7.3a1-DEV}. 
\end{enumerate}

\section{Syntax and Usage}
\hl{add jarray array and zeros}
This section covers each major OpenMP construct supported, along with the clauses that can be used in conjunction. The function will be briefly explained, along with any major differences from the original OpenMP version. Note that all directives are in the form of a Python comment that always starts with \texttt{\#omp} (we omitted the ``pragma" present in the C/C++ OpenMP since it has no meaning in Python). Syntax examples are also provided.



\subsection{Parallel}
The \texttt{parallel} directive has the effect of spawning threads to run the subsequent code block in parallel. It is the only directive that the \texttt{num\_threads()} clause can be used with. If the \texttt{num\_threads()} is not included, then the value returned from \texttt{os.cpu\_count()} will be used by default. The \texttt{parallel} directive can also include any variable scoping clause or the reduction clause.

\begin{lstlisting}[language=Python]
 from omp import *
 
#omp parallel num_threads(2)
	id = omp_get_thread_num()
	print('Hello from thread: ', id)
\end{lstlisting}



\subsection{For}
The \texttt{for} directive in OpenMP is a convenience directive for for-loop partitioning among threads. This directive splits the iterations among the available threads according to the schedule set by the \texttt{schedule()} clause (if not present, it will be set to static, with a chunk size of approximately the number of iterations divided by the number of threads, by default). Dynamic and guided scheduling is also supported. Because the for-loop construct in Python is really a for-each loop, we enforce a particular structure when using the \texttt{for} directive. The for-loop following a \texttt{for} directive must be of the form \texttt{for [single variable] in range([1, 2, or 3 parameters]):}. If one parameter (\texttt{n}) is passed to \texttt{range()}, then the returned range will be \texttt{[0, n-1]}. If two parameters are passed (\texttt{k}, and \texttt{n}), then the range will be \texttt{[k, n-1]}. Finally, if three parameters are passed (\texttt{k}, \texttt{n}, and \texttt{i}), then the range will be \texttt{[k, k+i, k+2i, k+3i, ..., n-1]}. This ensures that OpenMP for loops will behave as close to C-like for-loops as possible.

\begin{lstlisting}[language=Python]
 from omp import *
 
#omp parallel num_threads(2)
    #omp for schedule(dynamic, 5)
        for i in range(0, 100, 2):
            print('Iteration: ', i')
\end{lstlisting}



\subsection{Parallel For}
The parallel for directive combines the functions of the previous two directives into a single directive. It is functionally identical to the parallel directive, directly followed by the for directive.

\begin{lstlisting}[language=Python]
 from omp import *
 
#omp parallel for num_threads(2) schedule(guided, 10)
	for i in range(1, 100):
		print('Iteration: ', i)
\end{lstlisting}


 
 \subsection{Barrier}
 The barrier directive is used for synchronization. It ensures that no thread will continue past the barrier until all threads have made it to the barrier. An excellent example of this is the implicit barrier at the end of a for directive. By default, all of the threads will wait for the others to finish their iterations before continuing with the program.
  
 \begin{lstlisting}[language=Python]
 from omp import *
 
#omp parallel
	print('first print statement')
	#pragma omp barrier
	print('All threads have completed first print statement')
\end{lstlisting} 



 \subsection{Critical}
 The critical directive protects the critical section of a parallel block of code. A critical section is any code that could result in race conditions if it is executed by multiple threads in parallel. The critical directive is used to serialize this portion of code so that only one thread may execute it at a time. 
 
 \begin{lstlisting}[language=Python]
 from omp import *
 
sum = 0
#omp parallel for
	for i in range(10):
		#omp critical
			sum += 1
\end{lstlisting}



\subsection{Reduction} 
The reduction clause is a convenient method for assigning each thread a private variable, and then performing some operation across all of them at the end of the block and placing the result back in the variable with the same name from the outer scope. This works by providing a variable name and an operation as arguments to the clause. Then each thread can access the variable just like a private variable without worrying about synchronization. After the block executes, the result will automatically be placed into the original variable. The operations that are supported are as follows: +, -, *, \& (bit level AND), $|$ (bit level OR), \^{} (Bit level XOR), \&\& (logical AND), $||$ (logical OR), max, and min.

\begin{lstlisting}[language=Python]
 from omp import *
 
sum = 0
#omp parallel for reduction(+:sum)
	for i in range(10):
		sum += 1
\end{lstlisting}



 \subsection{Master/Single}
 Currently, Master and Single have the same effect. These directives ensure that only the thread with id 0 will execute the subsequent block. In a future update, we plan to differentiate between the two by allowing \texttt{single} blocks to be executed by the first thread that encounters the directive, regardless of whether or not it is thread 0.
 
 \begin{lstlisting}[language=Python]
 from omp import *
 
#omp parallel
	print('printed by all threads')
	#omp master
		print('only printed by thread 0')
	#omp single
		print('only printed by thread 0')
\end{lstlisting}
 
 \section{Runtime API}
All runtime functions exist in the omp module, which must be imported to use them. This can be done with ``\texttt{from omp import *}". The directory where this module exists is added to the python path automatically, so the user doesn't need to worry about where it is located. Currently our runtime API supports \hl{two (add omp\_get\_w\_time())} function calls. The first is \texttt{omp\_get\_thread\_num()}. This function returns a unique id corresponding to the thread that calls it. The ids are always assigned starting with 0 (the master thread) and then incrementing by one for each additional thread created. Therefore, the programmer can assume that whatever threads are available in a given parallel region have ids \texttt{0} through \texttt{p-1} where \texttt{p} is the number of threads executing the region.

The second function we provide is \texttt{omp\_get\_num\_threads()}. This function returns the number of available threads executing a parallel region.

\begin{enumerate}
\item \texttt{omp\_get\_thread\_num()} - returns integer thread id
\item \texttt{omp\_get\_num\_threads()} - returns integer representing the number of threads active in the current scope
\end{enumerate}

 \begin{lstlisting}[language=Python]
 from omp import *
 
#omp parallel num_threads(5)
	print('printed by thread ', omp_get_thread_num(), ' of ', \ omp_get_num_threads())
\end{lstlisting}




\end{document}





























